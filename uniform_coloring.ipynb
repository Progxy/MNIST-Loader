{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Definiamo una classe Ranger che utilizza l'Enum per rappresentare tre stati (ABOVE, UNDER e IN_RANGE) e tramite il metodo is_in_range\n",
        "#verifichiamo la posizione di un valore rispetto a un valore atteso (expected_value) e una percentuale di tolleranza (range_perc).\n",
        "from enum import Enum\n",
        "\n",
        "class Ranger(Enum):\n",
        "  ABOVE    = 1\n",
        "  UNDER    = 2\n",
        "  IN_RANGE = 3\n",
        "\n",
        "  def is_in_range(value, expected_value, range_perc, range_type):\n",
        "    range_diff = np.floor((expected_value / 100) * range_perc).astype(int)\n",
        "    if range_type == Ranger.ABOVE:\n",
        "      return value > expected_value\n",
        "    elif range_type == Ranger.UNDER:\n",
        "      return value < expected_value\n",
        "    elif range_type == Ranger.IN_RANGE:\n",
        "      return (value >= (expected_value - range_diff)) and (value <= (expected_value + range_diff))"
      ],
      "metadata": {
        "id": "ecn5xkHkhCXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self, model = \"\", dataset_prefix = \"\", labels_mapping = {}, labels_cnt = 0, labels_diff = 0, train = False, use_compression = False, enable_debug = False):\n",
        "        self.model = None\n",
        "        self.dataset_prefix = dataset_prefix\n",
        "        self.labels_mapping = labels_mapping\n",
        "        self.labels_cnt = labels_cnt\n",
        "        self.labels_diff = labels_diff\n",
        "        self.use_compression = use_compression\n",
        "        self.enable_debug = enable_debug\n",
        "        assert (model != \"\")\n",
        "        if train:\n",
        "          self.train_model()\n",
        "          self.save_model(self.model)\n",
        "        else: self.model = load_model(model)\n",
        "        pass\n",
        "\n",
        "    def train_model(self):\n",
        "        # Detect and configure the best available hardware (TPU, GPU, or CPU)\n",
        "        try:\n",
        "            # Try to detect a TPU\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "            strategy = tf.distribute.TPUStrategy(resolver)\n",
        "            print(\"Running on TPU\")\n",
        "        except ValueError:\n",
        "            # Fallback to GPU or CPU\n",
        "            if tf.config.list_physical_devices('GPU'):\n",
        "                strategy = tf.distribute.MirroredStrategy()  # Use all available GPUs\n",
        "                print(\"Running on GPU\")\n",
        "            else:\n",
        "                strategy = tf.distribute.get_strategy()  # Default strategy for CPU\n",
        "                print(\"Running on CPU\")\n",
        "\n",
        "        with strategy.scope():\n",
        "            # Load and preprocess the training data\n",
        "            x_train, y_train = self.load_emnist(\n",
        "                f\"{self.dataset_prefix}-train-labels-idx1-ubyte\",\n",
        "                f\"{self.dataset_prefix}-train-images-idx3-ubyte\"\n",
        "            )\n",
        "            x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalize and reshape\n",
        "            y_train = y_train - self.labels_diff  # Adjust labels to 0 index\n",
        "\n",
        "            x_test, y_test = self.load_emnist(\n",
        "                f\"{self.dataset_prefix}-test-labels-idx1-ubyte\",\n",
        "                f\"{self.dataset_prefix}-test-images-idx3-ubyte\"\n",
        "            )\n",
        "            x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalize and reshape\n",
        "            y_test = y_test - self.labels_diff  # Adjust labels to 0 index\n",
        "\n",
        "            # Build the CNN model\n",
        "            self.model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(256, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.4),\n",
        "                tf.keras.layers.Dense(self.labels_cnt, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # Compile the model\n",
        "            self.model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "        # Prepare the data pipeline\n",
        "        batch_size = 128 if isinstance(strategy, tf.distribute.TPUStrategy) else 64  # Adjust batch size for TPU\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "            .shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
        "            .batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
        "\n",
        "        # Evaluate the model on test data\n",
        "        test_loss, test_accuracy = self.model.evaluate(test_dataset)\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def save_model(self, model_name):\n",
        "      self.model.save(model_name)\n",
        "      return\n",
        "\n",
        "    # Classify a single processed image\n",
        "    def classify_image(self, image):\n",
        "        if self.model == None: self.train_model()\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        image = cv2.resize(image, (28, 28)).astype('float32')  # Resize to 28x28\n",
        "        if self.enable_debug:\n",
        "          print(\"Resized Image:\")\n",
        "          cv2_imshow(image)\n",
        "        image = image.reshape(1, 28, 28, 1)  # Reshape to model input shape\n",
        "\n",
        "        # Predict the class\n",
        "        prediction = self.model.predict(image)\n",
        "        predicted_label = np.argmax(prediction)\n",
        "        predicted_char = self.labels_mapping[predicted_label]  # Map label to character\n",
        "        print(f\"Predicted Label: {predicted_label}, Predicted Character: {predicted_char}\")\n",
        "        return\n",
        "\n",
        "    def load_emnist(self, label_path, image_path):\n",
        "        if self.use_compression:\n",
        "            label_path += \".gz\"\n",
        "            image_path += \".gz\"\n",
        "\n",
        "        labels = []\n",
        "        labels_data = b\"\"\n",
        "        with open(label_path, 'rb') as f:\n",
        "            labels_data = f.read()\n",
        "\n",
        "        if self.use_compression: labels_data = gzip.decompress(labels_data)\n",
        "        magic, size = struct.unpack('>II', labels_data[:8])\n",
        "        assert (magic == 2049)\n",
        "        print(f\"magic: {magic}, size: {size}\")\n",
        "        labels = np.frombuffer(labels_data[8:], dtype=np.uint8)\n",
        "\n",
        "        images = []\n",
        "        images_data = b\"\"\n",
        "        with open(image_path, 'rb') as f:\n",
        "            images_data = f.read()\n",
        "\n",
        "        if self.use_compression: images_data = gzip.decompress(images_data)\n",
        "        magic, size, rows, cols = struct.unpack('>IIII', images_data[:16])\n",
        "        assert (magic == 2051)\n",
        "        num_pixels = size * rows * cols\n",
        "        image_data = np.frombuffer(images_data[16:16+num_pixels], dtype=np.uint8)\n",
        "        images = image_data.reshape((size, rows, cols))\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VAn1f9i4i5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Convert all this methods in class methods, of the class \"Scrutineer\"\n",
        "\n",
        "def is_near(rect_a, rect_b):\n",
        "  contour_a, img_a, x_a, y_a, w_a, h_a = rect_a\n",
        "  contour_b, img_b, x_b, y_b, w_b, h_b = rect_b\n",
        "  v_diff = -((y_a + h_a) - y_b) if w_a > w_b else -((y_b - h_a) - y_a)\n",
        "\n",
        "  if w_a > w_b:\n",
        "    if not (x_a < x_b and (x_a + w_a) > (x_b + w_b)): return False\n",
        "  else:\n",
        "    if not (x_b < x_a and (x_b + w_b) > (x_a + w_a)): return False\n",
        "\n",
        "  if not (v_diff >= 0 and v_diff <= (max(h_a, h_b) / 2)): return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def reassemble_rects(img, rects):\n",
        "  for i in range(0, len(rects)):\n",
        "    for j in range(i + 1, len(rects)):\n",
        "      if is_near(rects[j], rects[i]):\n",
        "        # Merge contours\n",
        "        merged_contour = np.vstack((rects[i][0], rects[j][0]))\n",
        "        x, y, w, h = cv2.boundingRect(merged_contour)\n",
        "        rects.pop(j)\n",
        "        rects[i] = (merged_contour, img.copy()[y:y + h, x:x + w], x, y, w, h)\n",
        "        break\n",
        "  return rects\n",
        "\n",
        "def find_rect_from_bin(img, expected_w_perc, expected_h_perc, expected_diff_perc, range_type, enable_debug):\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "\n",
        "  # Preprocess the image using Otsu's thresholding after Gaussian filtering\n",
        "  blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "  _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "  closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "  if enable_debug:\n",
        "    print(\"Closing Image:\")\n",
        "    cv2_imshow(closing)\n",
        "\n",
        "  # Detect contours\n",
        "  contours, _ = cv2.findContours(closing, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  # Determine expected size\n",
        "  min_expected_h = np.ceil(expected_h_perc * (np.array(closing).shape[0] / 100)).astype(int)\n",
        "  min_expected_w = np.ceil(expected_w_perc * (np.array(closing).shape[1] / 100)).astype(int)\n",
        "  if range_type == Ranger.ABOVE:\n",
        "    print(f\"Expected size: ({min_expected_h}>) x ({min_expected_w}>)\")\n",
        "  elif range_type == Ranger.UNDER:\n",
        "    print(f\"Expected size: ({min_expected_h}<) x ({min_expected_w}<)\")\n",
        "  elif range_type == Ranger.IN_RANGE:\n",
        "    h_range_diff = np.floor((min_expected_h / 100) * expected_diff_perc).astype(int)\n",
        "    w_range_diff = np.floor((min_expected_w / 100) * expected_diff_perc).astype(int)\n",
        "    if enable_debug: print(f\"min_expected_h: {min_expected_h}, min_expected_w : {min_expected_w}\")\n",
        "    print(f\"Expected size: ({min_expected_h - h_range_diff} - {min_expected_h + h_range_diff}) x ({min_expected_w - w_range_diff} - {min_expected_w + w_range_diff})\")\n",
        "\n",
        "  rects = []\n",
        "  discarded = []\n",
        "  idx = 0\n",
        "  discarded_idx = 0\n",
        "  for contour in contours:\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "      if Ranger.is_in_range(w, min_expected_w, expected_diff_perc, range_type) and Ranger.is_in_range(h, min_expected_h, expected_diff_perc, range_type):\n",
        "        if enable_debug: print(f\"Rect match: {h} x {w} at {y} x {x}, index: {idx}\")\n",
        "        rects.append((contour, closing.copy()[y:y + h, x:x + w], x, y, w, h))\n",
        "        idx += 1\n",
        "      else:\n",
        "        if enable_debug: print(f\"Rect found: {h} x {w} at {y} x {x}, index: {discarded_idx}\")\n",
        "        discarded.append((closing.copy()[y:y + h, x:x + w], x, y, w, h))\n",
        "        discarded_idx += 1\n",
        "\n",
        "  rects = reassemble_rects(closing, rects)\n",
        "  rects_cnt = len(rects)\n",
        "  rects_idx = 0\n",
        "\n",
        "  print(\" -- Check for Written Sign -- \")\n",
        "  while rects_idx != rects_cnt:\n",
        "    contour, image, x, y, w, h = rects[rects_idx]\n",
        "    erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (18, 18))\n",
        "    eroded = cv2.erode(image.copy(), erosion_kernel, iterations = 1)\n",
        "\n",
        "    if enable_debug:\n",
        "      print(f\"Testing Rect {rects_idx}\")\n",
        "      cv2_imshow(image)\n",
        "\n",
        "    if not (is_a_written_sign(image, 2.8, enable_debug) or is_a_written_sign(eroded, 2.8, enable_debug)):\n",
        "      print(f\" -- Removing Rect {rects_idx} -- \")\n",
        "      print(\"Original Image:\")\n",
        "      is_a_written_sign(image, 2.8, True)\n",
        "      cv2_imshow(image)\n",
        "      print(\"After Erosion:\")\n",
        "      is_a_written_sign(eroded, 2.8, True)\n",
        "      cv2_imshow(eroded)\n",
        "      print(\" -- End Rect Removed -- \")\n",
        "      cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "      rects.pop(rects_idx)\n",
        "      rects_cnt -= 1\n",
        "      rects_idx -= 1\n",
        "\n",
        "    rects_idx += 1\n",
        "    if enable_debug: print(\"-------------------------\")\n",
        "\n",
        "  print(\"-----------------------------\")\n",
        "\n",
        "  # Sort the Cells and determine the number of elements per row and per column\n",
        "  grouped_rects = group_rects(rects)\n",
        "\n",
        "  return img, grouped_rects, discarded\n",
        "\n",
        "def find_rect_from_img(img_path, expected_w_perc, expected_h_perc, expected_diff_perc, range_type, enable_debug):\n",
        "  # Load the image\n",
        "  image = cv2.imread(img_path)  # Load the image\n",
        "  return find_rect_from_bin(image, expected_w_perc, expected_h_perc, expected_diff_perc, range_type, enable_debug)\n",
        "\n",
        "def get_matrix_list_shape(matrix_list):\n",
        "  return len(matrix_list), len(matrix_list[0])\n",
        "\n",
        "def get_row_y_pos(row):\n",
        "  contour, img, x, y, w, h = row[0]\n",
        "  return y\n",
        "\n",
        "def get_rect_x_pos(rect):\n",
        "  contour, img, x, y, w, h = rect\n",
        "  return x\n",
        "\n",
        "def group_rects(rects):\n",
        "  grouped_rects = []\n",
        "  for idx_rect, rect in enumerate(rects):\n",
        "    contour, img, x, y, w, h = rect\n",
        "    new_row = True\n",
        "    for idx, row in enumerate(grouped_rects):\n",
        "      contour_1, img_1, x_1, y_1, w_1, h_1 = row[0]\n",
        "      v_diff = abs(y - y_1)\n",
        "      if v_diff < max(h, h_1):\n",
        "        grouped_rects[idx].append(rect)\n",
        "        new_row = False\n",
        "        break\n",
        "    if new_row:\n",
        "      row = [rect]\n",
        "      grouped_rects.append(row)\n",
        "\n",
        "  grouped_rects = sorted(grouped_rects, key=get_row_y_pos)\n",
        "\n",
        "  for idx, row in enumerate(grouped_rects):\n",
        "    grouped_rects[idx] = sorted(row, key=get_rect_x_pos)\n",
        "\n",
        "  return grouped_rects\n",
        "\n",
        "def is_a_written_sign(img, threshold, enable_debug):\n",
        "  total_cnt = 0\n",
        "  white_cnt = 0\n",
        "  black_cnt = 0\n",
        "\n",
        "  for row in img:\n",
        "    for element in row:\n",
        "      if element == 255: white_cnt += 1\n",
        "      else: black_cnt += 1\n",
        "      total_cnt += 1\n",
        "\n",
        "  if white_cnt == 0 or black_cnt == 0:\n",
        "    if enable_debug: print(f\"total_cnt: {total_cnt}, black_cnt: {black_cnt}, white_cnt: {white_cnt}\")\n",
        "    return False\n",
        "\n",
        "  if enable_debug: print(f\"total_cnt: {total_cnt}, black_cnt: {black_cnt}, white_cnt: {white_cnt}, percentage_a: {black_cnt/white_cnt}\")\n",
        "\n",
        "  # TODO: Apply more complex and finer strategy to determine if it is noise, a random line or a written sign like a character\n",
        "  return black_cnt/white_cnt > threshold"
      ],
      "metadata": {
        "id": "WpzJdc7kABle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "enable_debug = False\n",
        "\n",
        "labels_mapping = {0: \"g\", 1: \"y\", 2: \"b\", 3: \"t\", 4: \"invalid\"}\n",
        "classifier = Classifier(\"my-small-model.keras\", \"./my-dataset\", labels_mapping, len(labels_mapping), train=True, use_compression=True, enable_debug=enable_debug)\n",
        "\n",
        "#                                                        <path to image>  - Percentages -   Range Type   -   Debug?\n",
        "image, grouped_cells, discarded = find_rect_from_img(\"TestImage_hpencil.png\", 7, 7, 100, Ranger.IN_RANGE, enable_debug)\n",
        "\n",
        "# The previous function returns the original image, the cells of the detected table in the same order, the elements discarded\n",
        "# From each cells the image that represent the character needs to be mapped to the right character\n",
        "\n",
        "print(\"\\nResult:\")\n",
        "\n",
        "(rows, cols) = get_matrix_list_shape(grouped_cells)\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"Matrix shape: ({rows}, {cols})\")\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "for idx_row, row in enumerate(grouped_cells):\n",
        "  print(f\"Row {idx_row}:\")\n",
        "  for idx, cell in enumerate(row):\n",
        "    contour, img, x, y, w, h = cell\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    print(f\"Element {idx}:\")\n",
        "    cv2_imshow(img)\n",
        "    classifier.classify_image(img)\n",
        "  print(\"-----------------------------------\")\n",
        "\n",
        "for idx, discard in enumerate(discarded):\n",
        "  img, x, y, w, h = discard\n",
        "  cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2) # Add the discarded ones to the image contours\n",
        "  if enable_debug:\n",
        "      print(f\"Discarded {idx}:\")\n",
        "      cv2_imshow(img)\n",
        "\n",
        "print(\" -- Image with Contours -- \")\n",
        "cv2_imshow(image)\n",
        "print(\"-----------------------------------\")"
      ],
      "metadata": {
        "id": "PugOVcABvS-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8624e8-d1e3-47c5-ac75-aa30f15fc679"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CPU\n",
            "magic: 2049, size: 495616\n",
            "magic: 2049, size: 57344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7744/7744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 94ms/step - accuracy: 0.9978 - loss: 0.0409 - val_accuracy: 0.6651 - val_loss: 126.1912\n",
            "Epoch 2/5\n",
            "\u001b[1m5453/7744\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 89ms/step - accuracy: 0.9923 - loss: 0.4134"
          ]
        }
      ]
    }
  ]
}